---
slug: cuda-implementation-of-huffman-coding
title: CUDA Implementation of Huffman Coding
authors: [zeyu]
tags: [cuda, compression]
---

Guide to implement Huffman coding using CUDA.

<!-- truncate -->

import PasswordProtection from '../../../src/components/PasswordProtection';

<PasswordProtection password="">

## Huffman Coding

Huffman coding is a lossless data compression algorithm that assigns shorter codes to more frequent symbols in a dataset. Theoretically, the average code length is equal to the entropy of the dataset.

The algorithm is based on the following steps:

1. Count the frequency of each symbol in the dataset.
2. Build a Huffman tree from the frequency of each symbol.
3. Assign codes to each symbol in the Huffman tree.

Key points:

- The codes generated by Huffman coding are prefix codes (or prefix-free codes). This means that no codeword (the bit string representing a symbol) is a prefix of any other codeword.

There is a Python [package](https://github.com/soxofaan/dahuffman) called `dahuffman` that implements the Huffman coding algorithm.

## Algorithm

### Encoding

<!-- KEY: Compute the Huffman tree of the data -->

**Computing the Huffman tree**

The average code length after Huffman coding is equal to the entropy of the data. Below is an example of the Huffman tree of the data. The codebook is printed as a table. The entropy of the data is approximately 2.65 bits per symbol.

```bash
Bits Code          Value Symbol
   3 000               0 10
   4 0010              2 9
   5 00110             6 8
   7 0011100          28 6
  13 0011101000000  1856 _EOF
  13 0011101000001  1857 0
  12 001110100001    929 1
  11 00111010001     465 2
  10 0011101001      233 3
   9 001110101       117 4
   8 00111011         59 5
   7 0011110          30 15
   7 0011111          31 7
   2 01                1 12
   3 100               4 14
   3 101               5 11
   2 11                3 13
```

**Representing Huffman trees in lookup tables**

The reason why we need to represent the Huffman tree in lookup tables is for fast decoding.

The objective is to have a lookup table that stores the codebook mapping codes to symbols.

```python
def get_luts_from_huffman_table(table):
    prefixes = ['']

    for symbol, (bits, decimal_val) in table.items():
        # Ignore the `_EOF` symbol
        if not isinstance(symbol, int):
            continue

        # `rjust` is used to pad 0s to the left of the string to make the string the 8-bit long
        binary_val = bin(decimal_val)[2:].rjust(bits, '0')

        # The prefix is n bytes long where n = (bits - 1) // 8
        prefix_len = (bits - 1) - (bits - 1) % 8
        prefix = binary_val[:prefix_len]
        vprint(f'bits: {bits:3d} | binary: {bin(decimal_val):>15} | decimal: {decimal_val:4d} | '
            f'padded binary: {binary_val:>15} | prefix: {prefix:>8}')

        if prefix not in prefixes:
            prefixes.append(prefix)

    prefixes.sort(key=len)
    vprint()
    vprint(f'prefixes: {prefixes}')
    vprint()

    # Initialize a lookup table where each row corresponds to a prefix
    luts = np.zeros((len(prefixes), 2 ** 8), dtype=np.uint8)

    # Iterate over each prefix to fill the lookup table
    for prefix_idx, prefix in enumerate(prefixes):
        bytes_dict = {}
        prefix_n_bytes = len(prefix) // 8
        prefix_n_bits = prefix_n_bytes * 8

        # Iterate over each code in the Huffman tree
        for symbol, (bits, decimal_val) in table.items():
            # Ignore the `_EOF` symbol
            if not isinstance(symbol, int):
                continue

            # If the code starts with the current prefix, then there are two cases
            binary_val = bin(decimal_val)[2:].rjust(bits, '0')
            if binary_val.startswith(prefix):
                # Case 1: When the first `prefix_n_bytes` bits of the code are the same as the prefix
                # time to map the code to the symbol
                if (bits - 1) // 8 == prefix_n_bytes:
                    # The suffix is the remaining bits after the prefix
                    # `ljust` is used to pad 0s to the right of the string to make the string the 8-bit long
                    suffix = binary_val[prefix_n_bits:]
                    padded_suffix = suffix.ljust(8, '0')
                    padded_suffix_int = int(padded_suffix, 2)

                    dict_key = padded_suffix_int
                    dict_value = symbol

                    vprint(f'prefix: {prefix:>8} | binary: {binary_val:>17} | suffix: {suffix:>8} | padded suffix: {padded_suffix:>8} | padded suffix int: {padded_suffix_int:>3d}')

                # Case 2: When the current prefix is a subset of the first n bytes of the code, and that n > `prefix_n_bytes`
                # map the next byte in the code to the longer prefix
                else:
                    assert (bits - 1) // 8 > prefix_n_bytes
                    next_byte = binary_val[prefix_n_bits:prefix_n_bits + 8]
                    next_byte_int = int(next_byte, 2)

                    # NOTE: due to the nature of the Huffman coding, if the current prefix is a subset of the first n bytes of the code,
                    # then the current prefix including the next byte MUST be in the prefixes list
                    prefix_and_next_byte = binary_val[:prefix_n_bits + 8]
                    prefix_next_byte_index = prefixes.index(prefix_and_next_byte)

                    dict_key = next_byte_int
                    dict_value = 2 ** 8 - prefix_next_byte_index

                    vprint(f'index: {prefix_next_byte_index}')
                    vprint(f'prefix: {prefix:>8} | binary: {binary_val:>17} | next byte: {next_byte:>8} | next byte int: {next_byte_int:>3d}')

                # Make sure that the mappings in case 1 and case 2 have no duplicates
                if dict_key in bytes_dict and bytes_dict[dict_key] != dict_value:
                    raise ValueError(f'Key {dict_key} already exists in {bytes_dict}')
                else:
                    bytes_dict[dict_key] = dict_value

        vprint(bytes_dict)

        for byte_node_int in range(2 ** 8):
            # `byte_node_int` could be either a suffix or a byte in the prefix
            if byte_node_int in bytes_dict:
                curr_val = bytes_dict[byte_node_int]
                luts[prefix_idx, byte_node_int] = curr_val

    # The last row of `luts` is the length of binary code for each symbol
    lens = np.zeros((1, 2 ** 8), dtype=np.uint8)
    for symbol, (bits, decimal_val) in table.items():
        if isinstance(symbol, int):
            lens[-1, symbol] = bits

    vprint(lens)

    # In summary, `luts` is a 2D array of shape `(len(prefixes) + 1, 256)`
    # Each row corresponds to a prefix, and the last row corresponds to the length of binary code for each possible symbol
    luts = torch.from_numpy(np.concatenate((luts, lens), axis=0))
    return luts


luts = get_luts_from_huffman_table(table)
```

_Q: Why choose bytes as the unit?_

_A:_ The choice of byte (8 bits) as the unit of processing is due to the hardware efficiency and the ease of parallelization.

- Byte-aligned operations are hardware-efficient as modern computers are optimized for byte-level processing.
- Byte-based operations are easier to parallelize on GPUs, which is crucial for the performance of this compression system.

### Decoding

## CUDA Kernel

### Preliminary Setup

**Defining macros**

Macros are a way to define constants or functions that can be used throughout the code.

- `#define` is a preprocessor directive that tells the compiler to replace one text with another before the code is compiled.
- `THREAD_ID` is the name of the macro we're creating. It's a custom name that makes the code more readable.
- `threadIdx.x` is what `THREAD_ID` will be replaced with. This is a built-in CUDA variable that gives each thread its unique identifier within a block.

In CUDA programming, when you run code on a GPU, it's executed by many threads in parallel. Each thread needs to know "who it is" so it can work on its own piece of data. `threadIdx.x` is like a thread's ID number within its block. `blockIdx.x` is the ID number of the block. `blockDim.x` is the number of threads in a block. `BYTES_PER_THREAD` is the number of bytes each thread will process.

```cpp
#define THREAD_ID           threadIdx.x
#define BLOCK_ID            blockIdx.x
#define N_THREADS           blockDim.x
#define BYTES_PER_THREAD    8
```

**Creating type aliases**

Type aliases are a way to give a nickname to a data type.

- `typedef` is a keyword that tells the compiler we're creating a new name for an existing type.
- `unsigned char` is the original type. An `unsigned char` is a data type that can store numbers from 0 to 255 (8 bits of data). The "unsigned" means it can only store positive numbers.
- `uint8_t` is the new name we're giving to `unsigned char`.

Similarly, `uint16_t`, `uint32_t`, and `uint64_t` are type aliases for `unsigned short`, `unsigned int`, and `unsigned long long` respectively.

```cpp
typedef unsigned char       uint8_t;
typedef unsigned short      uint16_t;
typedef unsigned int        uint32_t;
typedef unsigned long long  uint64_t;
```

**C-style linkag**

- The `extern "C"` keyword is used to tell the compiler that the function should be compiled using C-style linkage. This is important because CUDA uses a C-compatible calling convention. Let's break down why this is important:
  - Name Mangling: C++ compilers typically "mangle" (modify) function names to support features like function overloading. C compilers don't do this name mangling. `extern "C"` tells the compiler to use the original function name without any modifications.
  - Linking Compatibility: This makes the function callable from both C and C++ code. It's particularly important in CUDA because the GPU code needs to be callable from both C and C++ host code.
  - Why it's used in this CUDA file: The `decode` function in this file is a CUDA kernel that needs to be called from the host (CPU) code. Using `extern "C"` ensures that the function can be called from any C or C++ code without naming conflicts. This is especially important for CUDA programs where you often mix C++ host code with CUDA device code

```cpp
extern "C"
```

### Kernel Declaration

**Declaring the kernel function**

A kernel in CUDA (and GPU programming in general) is a special function that runs on the GPU. In CUDA, kernels are marked with the `__global__` keyword. When you call a kernel, it launches many threads in parallel. Each thread executes the same code but can work on different data.

Kernels can't return values directly. Instead, they operate on data in GPU memory. They're designed to perform the same operation on many pieces of data simultaneously.

To declare a kernel function, we should have the function name, the return type which is always `void` in CUDA, the arguments, and the function body.

- `__global__` is a keyword that tells the compiler that the function is a kernel.
- `void` means the function doesn't return any value.
- `decode` is the name of the kernel function.

In the arguments declaration for example `const uint8_t * __restrict__ luts`,

- `const` indicates that the data pointed to cannot be modified. The values in the array will remain unchanged during the kernel execution, which helps the compiler optimize the code.
- `uint8_t` is the type, unsigned 8-bit integer, of the argument, defined earlier in the code.
- `*` indicates this is a pointer that points to an array of `uint8_t` values, and the array is stored in the GPU memory. If not using `*`, the argument will be a copy of the data (we do not want this).
- `__restrict__` is a CUDA-specific keyword that tells the compiler that this pointer is the only way to access the memory it points to. This helps the compiler optimize the code by assuming no aliasing (no other pointer points to the same memory).
- `luts` is the name of the argument. It's a pointer to an array of `uint8_t` values stored in the GPU memory.

```cpp
__global__ void decode(
    const uint8_t* __restrict__  luts,             // (n_luts, 256)
    const uint8_t* __restrict__  encoded,          // (n_bytes,)
    const uint8_t* __restrict__  sign_mantissa,    // (n_elements,)
    const uint32_t* __restrict__ output_positions, // one entry per block
    const uint8_t* __restrict__  gaps,             // one entry per thread
    uint8_t* __restrict__        output,           // (n_elements,)
    const int                    n_luts,           // number of lookup tables
    const int                    n_bytes,          // number of bytes in the encoded data
    const int                    n_elements        // number of elements in the output data
) {
    // Do something
    if (threadIdx.x == 0 && blockIdx.x == 0) {
        printf("Hello world from first thread\n");
    }
}
```

### Registers and Shared Memory Allocation

**Register buffer**

Here we declare `register_buffer`, an unsigned 8-bit integer array that will be stored in the GPU registers. `[12]` means the array has 12 elements. Registers are the fastest type of memory in a GPU, much faster than global and shared memory. Each register is 32 bits and each thread has 32, 64, 256 registers depending on the GPU architecture. Newer version of GPUs have more registers per thread.

This buffer `register_buffer` is used to temporarily store data during the decoding process. It's declared at the start of the kernel function. Each thread gets its own copy of this buffer. The data in this buffer is processed and then used to generate the output.

```cpp
uint8_t register_buffer[12];
```

**Dynamic shared memory**

- `extern` is a keyword that indicates this is an external declaration, and the actual size of the array will be determined when the kernel is launched. This is required for dynamic shared memory allocation in CUDA.
- `__shared__` is a CUDA keyword that specifies this memory is in shared memory, which is
  - faster than global memory
  - slower than registers
  - shared among all threads in the same block
  - limited in size (typically 48 KB per block)
- `volatile` is a CUDA keyword that tells the compiler that this memory can be modified by other threads, preventing the compiler from optimizing away memory operations, which is important for shared memory to ensure thread synchronization works correctly.
- `uint8_t` is the type of the elements in the array, defined earlier in the code.
- `shared_mem[]` is the name of the array, whose size is not specified here (that's why it's `extern`). The size will be determined when the kernel is launched.
- `shared_mem` points to the very beginning of the shared memory block, it is the starting point from which all other memory locations are calculated. All pointer arithmetic starts from this base address.

```cpp
extern __shared__ volatile uint8_t shared_mem[];
```

**Creating an accumulator array in shared memory**

- Left side: `volatile uint32_t* accumulators`
  - `volatile`: Tells compiler this memory can be modified by other threads
  - `uint32_t`: 32-bit unsigned integer type defined earlier in the code
  - `*`: Indicates this is a pointer
  - `accumulators`: Name of the pointer variable
- Right side: `(volatile uint32_t*) shared_mem`
  - `(volatile uint32_t*)`: Type cast that converts the shared memory from `uint8_t*` to `uint32_t*`
  - `shared_mem`: the original shared memory array

What this line does to create memory view. It takes the original shared memory and reinterpret it as an array of 32-bit unsigned integers. Each element now takes 4 bytes instead of 1 byte. The same memory is used, but is now viewed in a different way.

Think of it like this:

- Original memory: A sequence of bytes (like a string of beads)
- After casting: The same memory viewed as larger 32-bit numbers (like grouping beads into sets of 4)
- `volatile`: Ensures all threads see the same values when they read/write

This is a common pattern in CUDA for thread coordination and data sharing within a block.

```cpp
volatile uint32_t* accumulators = (volatile uint32_t*) shared_mem;
```

The size of `accumulators` is determined by `N_THREADS` because each thread needs one accumulator.

**Creating a write buffer in shared memory**

- Left side: `volatile uint16_t* write_buffer`
  - `volatile`: Tells compiler this memory can be modified by other threads
  - `uint16_t`: 16-bit unsigned integer type defined earlier in the code
  - `*`: Indicates this is a pointer
  - `write_buffer`: Name of the pointer variable
- Right side: `(volatile uint16_t*) (shared_mem + N_THREADS * 4 + 4)` -> This calculates the starting address for the write buffer
  - `(volatile uint16_t*)`: Type cast to `uint16_t*` from `uint8_t*`
  - `(shared_mem + N_THREADS * 4 + 4)` is a pointer arithmetic expression that calculates the starting address for the write buffer
    - `shared_mem`: base address of shared memory
    - `N_THREADS * 4`: skips past the accumulators array, as `N_THREADS` is the number of threads in a block and each accumulator is 4 bytes
    - `+ 4`: additional 4-byte offset to skip the 4-byte gap between the accumulators array and the write buffer array

Then the memory layout looks like this:

```text
shared_mem: [accumulators array][4 bytes][write_buffer array]
            ^                  ^        ^
            |                  |        |
            |                  |        write_buffer starts here
            |                  4-byte gap
            accumulators array (N_THREADS * 4 bytes)
```

The purpose of this line is to create a write buffer in shared memory for storing output data.

```cpp
volatile uint16_t* write_buffer  = (volatile uint16_t*) (shared_mem + N_THREADS * 4 + 4);
```

FP8: If your output data is 8-bit, then you can write the following line instead:

```cpp
volatile uint8_t* write_buffer  = shared_mem + N_THREADS * 4 + 4;
```

### Data Loading

**Locate thread ID**

Recall that:

```cpp
#define THREAD_ID           threadIdx.x
#define BLOCK_ID            blockIdx.x
#define N_THREADS           blockDim.x
#define BYTES_PER_THREAD    8
```

Here we calculate the global thread ID as the following line. The purpose is to calculate a unique global thread ID for each thread across all blocks.

```cpp
const int global_thread_id = BLOCK_ID * N_THREADS + THREAD_ID;
```

Here is how the blocks and threads are organized in a grid. In CUDA, block ID and thread ID start from 0.

```text
Grid: [Block 0][Block 1][Block 2][...][Block m]
Block 0: [Thread 0][Thread 1][Thread 2][...][Thread n-1]
Block 1: [Thread 0][Thread 1][Thread 2][...][Thread n-1]
Block 2: [Thread 0][Thread 1][Thread 2][...][Thread n-1]
...
Block m: [Thread 0][Thread 1][Thread 2][...][Thread n-1]
```

**Load some bytes of encoded data into a thread's register buffer**

- Condition check (prevents threads from accesssing beyond the input data)
  - `global_thread_id * BYTES_PER_THREAD < n_bytes` is to check if this thread has data to process
  - `BYTES_PER_THREAD` is the number of bytes each thread will process, which is 8 defined in the macros
  - `n_bytes` is the total size of the encoded data
- Data loading
  - `register_buffer[0] = encoded[global_thread_id * BYTES_PER_THREAD];`
  - Load one byte from the encoded data `encoded` into the first element of `register_buffer`
  - The position is calculated using `global_thread_id * BYTES_PER_THREAD`

```cpp
if (global_thread_id * BYTES_PER_THREAD < n_bytes) {
  register_buffer[0] = encoded[global_thread_id * BYTES_PER_THREAD];
}
```

The following code loads 12 bytes of encoded data into the register buffer. Why there are 12 bytes? Because we set `BYTES_PER_THREAD` to 8 in the macros, and the addition 4 bytes is loaded because during the decoding process, decoding sometimes needs to lood at multiple bytes after the current byte to determine the correct symbol. And the largest possible code length is 32 bits, so we need to load 4 bytes.

```cpp
if (global_thread_id * BYTES_PER_THREAD < n_bytes) {
    register_buffer[0] = encoded[global_thread_id * BYTES_PER_THREAD];
}
if (global_thread_id * BYTES_PER_THREAD + 1 < n_bytes) {
    register_buffer[1] = encoded[global_thread_id * BYTES_PER_THREAD + 1];
}
if (global_thread_id * BYTES_PER_THREAD + 2 < n_bytes) {
    register_buffer[2] = encoded[global_thread_id * BYTES_PER_THREAD + 2];
}
if (global_thread_id * BYTES_PER_THREAD + 3 < n_bytes) {
    register_buffer[3] = encoded[global_thread_id * BYTES_PER_THREAD + 3];
}
if (global_thread_id * BYTES_PER_THREAD + 4 < n_bytes) {
    register_buffer[4] = encoded[global_thread_id * BYTES_PER_THREAD + 4];
}
if (global_thread_id * BYTES_PER_THREAD + 5 < n_bytes) {
    register_buffer[5] = encoded[global_thread_id * BYTES_PER_THREAD + 5];
}
if (global_thread_id * BYTES_PER_THREAD + 6 < n_bytes) {
    register_buffer[6] = encoded[global_thread_id * BYTES_PER_THREAD + 6];
}
if (global_thread_id * BYTES_PER_THREAD + 7 < n_bytes) {
    register_buffer[7] = encoded[global_thread_id * BYTES_PER_THREAD + 7];
}
if (global_thread_id * BYTES_PER_THREAD + 8 < n_bytes) {
    register_buffer[8] = encoded[global_thread_id * BYTES_PER_THREAD + 8];
}
if (global_thread_id * BYTES_PER_THREAD + 9 < n_bytes) {
    register_buffer[9] = encoded[global_thread_id * BYTES_PER_THREAD + 9];
}
if (global_thread_id * BYTES_PER_THREAD + 10 < n_bytes) {
    register_buffer[10] = encoded[global_thread_id * BYTES_PER_THREAD + 10];
}
if (global_thread_id * BYTES_PER_THREAD + 11 < n_bytes) {
    register_buffer[11] = encoded[global_thread_id * BYTES_PER_THREAD + 11];
}
```

Then synchronize the threads to ensure all threads have loaded their data before proceeding. `__syncthreads()` is a CUDA synchronization barrier.

```text
Thread 0: Load data → __syncthreads() → Wait for others → Continue
Thread 1: Load data → __syncthreads() → Wait for others → Continue
Thread 2: Load data → __syncthreads() → Wait for others → Continue
Thread 3: Load data → __syncthreads() → Wait for others → Continue
```

```cpp
__syncthreads();
```

If we do not have `__syncthreads()`, then the threads will not wait for each other to finish loading their data and proceed to the next steps.

### Buffer Declaration and Memory Views

`alignas(8)` forces the buffer to start at the memory address divisible by 8, and this is called 8-byte alignment. Below is an example of how the memory access looks like when using `alignas(8)` and without `alignas(8)`. `buffer` is used for 64-bit operations later in the code so that's why we use `alignas(8)` to improve the efficiency.

```text
Aligned (with alignas(8)):
[64-bit value] - Can be read in one operation

Unaligned (without alignas(8)):
[part of 64-bit value][rest of 64-bit value] - Might need two operations
```

Here `buffer` is an array consisting of 12 `uint8_t` elements. We need a buffer to do manipulation on the data.

```cpp
alignas(8) uint8_t buffer[12];
```

- Left side: `uint64_t &long_buffer`:
  - `uint64_t`: 64-bit unsigned integer type defined earlier in the code
  - `&` creates a reference (not a pointer)
    - reference must be initialized when declared, and cannot be changed to refer to a different memory location
    - pointer instead, can be null when initialized and can be changed to refer to a different memory location
  - This allows us to modify the value directly through the reference
- Right side: `*reinterpret_cast<uint64_t *>(buffer)`:
  - `buffer` is an array of 12 `uint8_t` elements
  - when `buffer` is used in the expression, the array automatically decays to a pointer to the first element, which is a C/C++ feature where arrays automatically convert to pointers when used in expressions.
  - `reinterpret_cast<uint64_t*>` is a C++ type cast operator that tells the compiler to reinterpret the memory as a different type. No acctual data conversion happens, it is just a different way of looking at the same memory. What it does is to take `buffer` which is a `uint8_t*` pointer after array decay, and reinterpret it as a `uint64_t*` pointer. This means we are now viewing 8 consecutive bytes as one 64-bit value, and the remaining bytes are not used.
  - `*` dereferences the pointer to get the actual 64-bit value, this gives us access to the first 8 bytes as a single 64-bit value

```text
buffer: [byte0][byte1][...][byte7][byte8][byte9][byte10][byte11]
        ^ long_buffer (8 bytes)   ^ remaining 4 bytes
```

```cpp
uint64_t &long_buffer  = *reinterpret_cast<uint64_t*>(buffer);
```

- `long_buffer` can only access and modify the first 8 bytes, the remaining 4 bytes are used by other variables
- `buffer + 8` is a pointer arithmetic expression that calculates the starting address for the `int_buffer` and `short_buffer`
  - `buffer` is an array of 12 bytes, so `buffer + 8` moves the pointer 8 bytes forward, which skips the first 8 bytes that are used by `long_buffer`
- `int_buffer` and `short_buffer` are also created using `reinterpret_cast` to view the remaining bytes as 32-bit and 16-bit values respectively.

```text
buffer: [byte0][byte1][...][byte7][byte8][byte9][byte10][byte11]
        ^ long_buffer (8 bytes)   ^ int_buffer (4 bytes)
                                  ^ short_buffer (2 bytes)
```

```cpp
uint32_t &int_buffer   = *reinterpret_cast<uint32_t*>(buffer + 8);
uint16_t &short_buffer = *reinterpret_cast<uint16_t*>(buffer + 8);
```

### Gap Processing and Buffer Setup

**Load gap values**

```cpp
#define GAP_BIT_SIZE 5
#define BIT_PER_BYTE 8
```

- `buffer[8]` is the 9th byte of the buffer, and `buffer[9]` is the 10th byte of the buffer
- `global_thread_id * GAP_BIT_SIZE / BIT_PER_BYTE` calculates which byte in the `gaps` array to read.
- Need both `+ 1` and `+ 0` because it is possible that the gap value for the current thread is stored in two consecutive bytes in the `gaps` array. Here is an example where you can see that the gap value for thread 1 is stored in both `gaps[0]` and `gaps[1]`.

```text
[bit0][bit1][bit2][bit3][bit4][bit5][bit6][bit7][bit8][bit9][...]
^                             ^                 ^
|                             |                 |
|                             |                 |
thread 0 gap                  thread 1 gap      |
byte 0                                          byte 1
```

```cpp
buffer[8] = gaps[global_thread_id * GAP_BIT_SIZE / BIT_PER_BYTE + 1];
buffer[9] = gaps[global_thread_id * GAP_BIT_SIZE / BIT_PER_BYTE];
```

**Extract gap values**

Using the example, let's get the gap value for thread 1, we want to get bit5 to bit9

```text
[bit0][bit1][bit2][bit3][bit4][bit5][bit6][bit7][bit8][bit9][...][bit15]
^                             ^                 ^
|                             |                 |
|                             |                 |
thread 0 gap                  thread 1 gap      |
byte 0                                          byte 1
```

- `short_buffer` is a reference to the 16-bit value in `buffer` starting from the 9th byte
- `global_thread_id * GAP_BIT_SIZE % BIT_PER_BYTE` is `1 * 5 % 8 = 5`, and `11 - 5 = 6`, and `short_buffer` will shift right by 6 bits, and the rightmost bit will be bit9
- `& 0x1f` is to mask the result to get the right most 5 bits, because `0x1f` is `00011111` in binary, and the and operation will keep only the right most 5 bits.

```cpp
const uint8_t gap = (short_buffer >> (11 - (global_thread_id * GAP_BIT_SIZE % BIT_PER_BYTE))) & 0x1f;
```

FP8: In FP8 the gap value is 4 bits, so it is impossible for a gap value to be stored in two consecutive bytes. We can change the kernel accordingly by only loading one byte of gap value to the buffer and then shift right by `4 - (global_thread_id * GAP_BIT_SIZE % BIT_PER_BYTE)`.

**Initialize a counter**

The `thread_counter` serves as a local counter for each thread to keep track of how many elements have been decoded. It is declared as `uint32_t` to ensure it can handle a large number.

Later in the code, this counter is incremented each time the thread successfully decodes an element. The final value of this counter is used to

- calculate the total number of elements decoded by all threads
- help coordinate between threads for writing the decoded data to the output buffer
- determine the correct output positions for each thread's decoded data

```cpp
uint32_t thread_counter = 0;
```

**Copy data**

TODO: Is reverse order correct?

```python
# Shift code bits for the current symbol into the buffer at the rightmost position
buffer = (buffer << bits) + decimal_val

# When the buffer is larger than 8 bits, extract the leftmost 8 bits
while size >= 8:
    byte = buffer >> (size - 8)
    encoded.append(byte)
```

- The data in `register_buffer` was loaded from the input `encoded` array.
- The code needs to process this data in reverse byte order (endianness conversion)
- This is because in the encoding process, we shifts new bits into the rightmost position and extract bytes from the leftmost position, and appends these bytes to the encoded array. This means that in the encoded array, the first byte contains the most significant bits, and the last byte contains the least significant bits.

```cpp
buffer[0] = register_buffer[7];
buffer[1] = register_buffer[6];
buffer[2] = register_buffer[5];
buffer[3] = register_buffer[4];
buffer[4] = register_buffer[3];
buffer[5] = register_buffer[2];
buffer[6] = register_buffer[1];
buffer[7] = register_buffer[0];
```

`long_buffer <<= gap` shifts the `long_buffer` left by `gap` bits. This is because the first `gap` bits in the current `long_buffer` have already been used for decoding the previous symbol.

```cpp
long_buffer <<= gap;
```

Now finally we have the actual huffman code in the `long_buffer` and we can start decoding.

### Decoding

Declare a variable to store the number of free bits in `long_buffer`. `free_bits` is the number of bits not used for decoding.

```cpp
uint8_t free_bits = gap;
uint8_t decoded;
```

**Initial Decoding Loop**

1. Process the first 32 bits of the input data
1. Uses the LUT to decode variable-length codes
1. Accumulates the number of decoded symbols in `thread_counter`

```cpp
while (free_bits < 32) {
  decoded = __ldg(&luts[long_buffer >> 56]);

  if (decoded >= 240) {
    decoded = __ldg(&luts[256 * (256 - decoded) + ((long_buffer >> 48) & 0xff)]);
    if (decoded >= 240) {
      decoded = __ldg(&luts[256 * (256 - decoded) + ((long_buffer >> 40) & 0xff)]);
      if (decoded >= 240) {
        decoded = __ldg(&luts[256 * (256 - decoded) + ((long_buffer >> 32) & 0xff)]);
      }
    }
  }
  thread_counter += 1;
  decoded = __ldg(&luts[256 * (n_luts - 1) + decoded]);
  long_buffer <<= decoded;
  free_bits += decoded;
}
```

- `long_buffer >> 56`: `long_buffer` is a 64-bit value and shifting right by 56 bits extract the most significant byte
- `&luts[long_buffer >> 56]`: uses the extracted byte as an index into the LUT array, and the LUT contains pre-computed decoding values
- `__ldg`: CUDA intrinsic for loading from global memory, but cache the loaded value in L1 cache. L1 cache is a small, fast memory that is used to store frequently accessed data.

```cpp
decoded = __ldg(&luts[long_buffer >> 56]);
```

The value 240 (0xF0) is a special marker in the LUT that indicates:

- If `decoded < 240`, we have a direct symbol value (0-239)
- If `decoded >= 240`, we need to look at more bits to get the complete code

In the LUT design, values 0-239 are used for actual decoded symbols, and values 240-255 are used as indices for secondary lookups. when we get a value greater than or equal to 240, it means the first byte alone wasn't enough to determine the symbol.

The next line `decoded = __ldg(&luts[256 * (256 - decoded) + ((long_buffer >> 48) & 0xff)]);`

- `(long_buffer >> 48) & 0xff`: `long_buffer >> 48` shifts right by 48 bits to get the second byte and `& 0xff` is to mask to keep only the lowest 8 bits
- `256 * (256 - decoded)`: `256 - decoded` give us the index into the next LUT and `256 *` shifts the index to the correct row in the LUT
- The final index is `256 * (256 - decoded) + ((long_buffer >> 48) & 0xff)`:
  - Points to a specific entry in the LUT
  - Uses both the next LUT index and the second byte value
  - This either gives us a direct symbol value or another index for the next lookup if >= 240.
  - `256 * (256 - decoded)` calculates the row offset, and `+ ((long_buffer >> 48) & 0xff)` calculates the column offset.
- TODO: verifiy this -> The value `240` is empirically chosen based on the statistics of bfloat16 weights - for FP8, we can use `17` instead because the biggest possible value for 4 bit exponent is 16.

LUT structure:

- The LUT is a 2D array where each row has 256 entries
- Each row corresponds to a different prefix
- The first rwo is for single-byte lookups
- Subsequent rows are for multi-byte lookups
- The last row of the LUT contains the bit lengths for each possible symbol value

Here we have a 4-level lookup for bfloat16 weights, but for FP8, we can just have a 2-level lookup.

```cpp
if (decoded >= 240) {
  decoded = __ldg(&luts[256 * (256 - decoded) + ((long_buffer >> 48) & 0xff)]);
  if (decoded >= 240) {
    decoded = __ldg(&luts[256 * (256 - decoded) + ((long_buffer >> 40) & 0xff)]);
    if (decoded >= 240) {
      decoded = __ldg(&luts[256 * (256 - decoded) + ((long_buffer >> 32) & 0xff)]);
    }
  }
}
```

When we have a decoded value, then we increment the `thread_counter` by 1, and the get the bit length of the symbol by looking up the last row of the LUT which contains the bit lengths for each possible symbol value.

- `256 * (n_luts - 1)` is the row offset for the last row and `+ decoded` now contains the symbol value, is now the column offset.

```cpp
thread_counter += 1;
decoded = __ldg(&luts[256 * (n_luts - 1) + decoded]);
```

now `decoded` contains the bit length of the symbol, and we shift the `long_buffer` left by the bit length of the symbol and update the `free_bits` by adding the bit length of the symbol.

```cpp
long_buffer <<= decoded;
free_bits += decoded;
```

**Transition between the first and second phase**

```cpp
buffer[8]   = register_buffer[11];
buffer[9]   = register_buffer[10];
buffer[10]  = register_buffer[9];
buffer[11]  = register_buffer[8];
```

- `long_buffer` is a 64-bit value containing the bits we are currently processing, after the initial decoding loop, it has some remaining bits
- `int_buffer`, a 32-bit value containing the next chunk of data, and we need to add these bits to `long_buffer` to continue decoding.
- the operation `static_cast<uint64_t>(int_buffer)` converts 32-bit to 64-bit, and then shift left by `free_bits - 32` bits to align the bits with `long_buffer`

```cpp
long_buffer |= static_cast<uint64_t>(int_buffer) << (free_bits - 32);
free_bits -= 32;
```

These lines handle the transition between the first and second phase of decoding.

- Frist line `long_buffer |= static_cast<uint64_t>(int_buffer) << (free_bits - 32);`:
  - After the first phase, `long_buffer` has some remaining bits (less than 32)
  - `int_buffer` contains the next 32 bits of data
  - `free_bits - 32` tells how many extra bits we used beyond the 32 bit
  - the next 32 bits should shift left by this number of bits to align with `long_buffer` which has already been shifted left by the `free_bits` bits.
  - `static_cast` is a C++ type conversion operator that performs explicit type conversion at compile time, and `static_cast<uint64_t>(int_buffer)` converts the 32-bit `int_buffer` to a 64-bit value by add 0s to the most significant 32 bits.
- Second line `free_bits -= 32;` updates the bit counter after the first phase, and now `free_bits` is less than 32.

**2nd Loop**

- `free_bits / 8` converts the number of bits we've processed into bytes and `4` is the number of bytes we have already processed in the first phase.
- the condition `4 + free_bits / 8 < BYTES_PER_THREAD` ensures we don't process more than the configured number of bytes per thread.
- then do the same decoding process as the first loop, but now we are processing the remaining bits in `long_buffer` and `int_buffer`

```cpp
while (4 + free_bits / 8 < BYTES_PER_THREAD) {
  decoded = __ldg(&luts[long_buffer >> 56]);
  if (decoded >= 240) {
    decoded = __ldg(&luts[256 * (256 - decoded) + ((long_buffer >> 48) & 0xff)]);
    if (decoded >= 240) {
      decoded = __ldg(&luts[256 * (256 - decoded) + ((long_buffer >> 40) & 0xff)]);
      if (decoded >= 240) {
        decoded = __ldg(&luts[256 * (256 - decoded) + ((long_buffer >> 32) & 0xff)]);
      }
    }
  }
  thread_counter += 1;
  decoded = __ldg(&luts[256 * (n_luts - 1) + decoded]);
  long_buffer <<= decoded;
  free_bits += decoded;
}
```

**Coordination between threads in a block**

Handle the coordination between threads in a block.

- For thread 0 (the first thread in the block), it takes the block's starting position, and adds its own count `thread_counter` and store it in `accumulators[0]`, this gives us the starting position for the next block.
- For all other threads, they simply store their individual `thread_counter` values, these values represent how many symbol each thread has decoded.
- `__syncthreads()` ensures all threads have finished writing to `accumulators` before any thread proceeds to the next step.

This is part of the parallel processing coordination where:

- Each thread keeps track of how many symbols it decoded-
- Thread 0 also tracks the starting position for the next block
- These values will be used to determine where each thread writes its decoded data

```cpp
if (THREAD_ID == 0) {
  accumulators[0] = position_offsets[BLOCK_ID] + thread_counter;
} else {
  accumulators[THREAD_ID] = thread_counter;
}
__syncthreads();
```

```cpp
int i;
for (i = 2; i <= N_THREADS; i <<= 1) {
  if (((THREAD_ID + 1) & (i - 1)) == 0) {
    accumulators[THREAD_ID] += accumulators[THREAD_ID - (i >> 1)];
  }
  __syncthreads();
}
```

## Profiling and Optimization

</PasswordProtection>
